# ============================================================
# Kalshi Market Data Collector — Configuration
# ============================================================

kalshi:
  # API credentials: set via env vars KALSHI_API_KEY_ID and KALSHI_PRIVATE_KEY_PATH
  # (see .env.example in this directory)
  private_key_path: "/home/kaitores/.kalshi/kalshi_api_key.txt"

  base_url: "https://api.elections.kalshi.com/trade-api/v2"
  ws_url:   "wss://api.elections.kalshi.com/trade-api/ws/v2"

# Series prefixes to track (uppercase).  At startup the collector queries the
# Kalshi API for currently-open events in each series and picks the active one
# automatically — no manual date updates needed.
# Example: KXHIGHCHI  →  auto-resolves to KXHIGHCHI-26FEB18 (or whatever is open today)
event_series:
  - "KXHIGHCHI"
  - "KXHIGHNY"

# Optional: pin specific dated event tickers here (legacy / override).
# events:
#   - "KXHIGHCHI-26FEB18"

# Weather stations to fetch temperature data for.
# Auto-resolved from event_series above if not specified.
# Use 4-letter ICAO codes matching the Kalshi resolution stations.
# weather_stations:
#   - "KMDW"    # Chicago-Midway  (KXHIGHCHI)
#   - "KNYC"    # Central Park    (KXHIGHNY)

# ----------------------------------------------------------
# Snapshot collection
# ----------------------------------------------------------
collection:
  # Baseline: snapshot every N seconds regardless of price activity.
  interval_seconds: 60

  # Spike detection: snapshot immediately when any contract's yes_bid, yes_ask,
  # or last_price moves by ≥ threshold cents since the *last snapshot*.
  # Compared against the last-snapshotted price (not the last WS message),
  # so cumulative moves during cooldown are never missed.
  spike_threshold_cents: 3

  # Min seconds between event-driven snapshots (prevents burst writes when
  # multiple contracts spike simultaneously or prices oscillate).
  spike_cooldown_seconds: 2

  # Max orderbook depth saved per side (yes/no) per contract per snapshot.
  # Levels are kept by best price (highest bids first for yes, no sides).
  # Set to 0 to save all levels.
  max_orderbook_depth: 5

  # Delta compression: write full orderbook baselines every N snapshots,
  # and only changed/removed levels in between. Saves ~80-90% of OB rows.
  # Set to 1 to disable (every snapshot is a baseline, i.e. original behavior).
  baseline_every_n_snapshots: 60

# ----------------------------------------------------------
# Storage
# ----------------------------------------------------------
storage:
  # Relative to this config file's directory.
  data_dir: "data"

  # How often to flush in-memory buffers to parquet (seconds).
  flush_interval_seconds: 300

# ----------------------------------------------------------
# LDM — Unidata Local Data Manager (real-time weather ingest)
# ----------------------------------------------------------
# The LDM runs as a separate Docker container and receives surface
# observations in real time from the Unidata IDD relay network.
# Configuration here is informational; the actual LDM config files
# are in the ldm/ directory (ldmd.conf, pqact.conf, etc.).
ldm:
  # FQDN or public IP of this LDM node (must match upstream ALLOW entry).
  # Set via LDM_HOSTNAME env var in docker-compose.yml.
  hostname: "changeme.example.com"

  # Upstream LDM server(s) for data requests.
  upstream: "idd.unidata.ucar.edu"

  # Feedtype to request (IDS|DDPLUS = domestic surface observations).
  feedtype: "IDS|DDPLUS"

  # Product filter regex (SA = routine METAR, SP = SPECI).
  product_filter: "^S[AP]"

  # Contact email for requesting upstream access:
  # support-idd@unidata.ucar.edu

synoptic:
  stations:
    - KATL1M
    - KAUS1M
    - KBOS1M
    - KDCA1M
    - KDEN1M
    - KDFW1M
    - KHOU1M
    - KLAS1M
    - KLAX1M
    - KMDW1M
    - KMIA1M
    - KMSP1M
    - KMSY1M
    - KOKC1M
    - KPHL1M
    - KPHX1M
    - KSAT1M
    - KSEA1M
    - KSFO1M
  vars:
    - air_temp
